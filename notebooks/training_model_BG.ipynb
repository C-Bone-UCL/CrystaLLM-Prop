{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uccacbo/miniconda3/envs/crystallm_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from crystallm import (\n",
    "    GPT,\n",
    "    GPTConfig,\n",
    "    CIFTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/uccacbo/CrystaLLM'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check current directory\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "# change directory to where the data is\n",
    "os.chdir('/home/uccacbo/CrystaLLM')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the tokenizer: 372\n"
     ]
    }
   ],
   "source": [
    "from crystallm import CIFTokenizer  # Assuming CIFTokenizer is defined in crystallm\n",
    "\n",
    "# Load the tokenizer (assuming a similar API)\n",
    "tokenizer = CIFTokenizer()\n",
    "\n",
    "# Assuming the tokens are stored in a dictionary-like attribute\n",
    "if hasattr(tokenizer, 'token_to_id'):\n",
    "    num_tokens = len(tokenizer.token_to_id)\n",
    "    print(f\"Number of tokens in the tokenizer: {num_tokens}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crystallm_v1_large/\n",
      "crystallm_v1_large/ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "# untar crystallm_v1_large.tar.gz\n",
    "!tar -xzvf crystallm_v1_large.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration:\n",
      "out_dir: pretrained_models/large_model_untouched\n",
      "ckpt_out_dir: finetuned_models/BG_LoRA_test\n",
      "eval_interval: 2\n",
      "log_interval: 2\n",
      "eval_iters_train: 2\n",
      "eval_iters_val: 2\n",
      "eval_only: false\n",
      "always_save_checkpoint: false\n",
      "init_from: resume\n",
      "wandb_log: true\n",
      "wandb_project: crystallm_CIF_BG_tests\n",
      "wandb_run_name: BG_large_LoRA\n",
      "dataset: CIF_BG_proj/BG_large_tokens\n",
      "gradient_accumulation_steps: 4\n",
      "batch_size: 4\n",
      "block_size: 2048\n",
      "n_layer: 12\n",
      "n_head: 12\n",
      "n_embd: 1024\n",
      "dropout: 0.1\n",
      "bias: false\n",
      "learning_rate: 0.0001\n",
      "max_iters: 4\n",
      "weight_decay: 0.1\n",
      "beta1: 0.9\n",
      "beta2: 0.99\n",
      "grad_clip: 1.0\n",
      "decay_lr: true\n",
      "warmup_iters: 100\n",
      "lr_decay_iters: 50\n",
      "min_lr: 1.0e-05\n",
      "device: cuda\n",
      "dtype: bfloat16\n",
      "compile: true\n",
      "underrep_p: 0.0\n",
      "validate: true\n",
      "codecarbon: false\n",
      "tracker_project: crystallm\n",
      "metrics_dir: comp_metrics\n",
      "CarbonTracker: false\n",
      "LoRA_rank: 4\n",
      "LoRA_alpha: 4\n",
      "finetune_method: LoRA\n",
      "sanity_check: true\n",
      "\n",
      "Resuming from pretrained_models/large_model_untouched...\n",
      "Reading start indices from CIF_BG_proj/BG_large_tokens/starts.pkl...\n",
      "Found vocab_size = 372 (inside CIF_BG_proj/BG_large_tokens/meta.pkl)\n",
      "Resuming training from pretrained_models/large_model_untouched...\n",
      "number of parameters: 201.74M\n",
      "Vocabulary size mismatch detected: checkpoint has 371, dataset has 372\n",
      "Resizing token embeddings from 371 to 372\n",
      "Replacing linear layers with LoRA layers\n",
      "Setting requires_grad for LoRA parameters\n",
      "no weight decay applied to transformer.h.0.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.0.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.0.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.0.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.1.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.1.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.1.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.1.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.2.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.2.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.2.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.2.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.3.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.3.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.3.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.3.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.4.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.4.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.4.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.4.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.5.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.5.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.5.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.5.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.6.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.6.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.6.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.6.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.7.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.7.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.7.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.7.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.8.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.8.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.8.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.8.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.9.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.9.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.9.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.9.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.10.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.10.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.10.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.10.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.11.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.11.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.11.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.11.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.12.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.12.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.12.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.12.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.13.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.13.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.13.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.13.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.14.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.14.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.14.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.14.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.15.attn.c_attn.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.15.attn.c_attn.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.15.attn.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.15.attn.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_fc.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_fc.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_proj.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_proj.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to lm_head.lora.A, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to lm_head.lora.B, it's a <class 'crystallm._model.GPT'>\n",
      "no weight decay applied to transformer.h.0.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.0.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.0.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.0.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.1.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.1.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.1.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.1.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.2.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.2.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.2.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.2.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.3.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.3.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.3.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.3.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.4.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.4.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.4.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.4.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.5.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.5.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.5.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.5.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.6.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.6.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.6.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.6.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.7.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.7.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.7.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.7.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.8.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.8.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.8.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.8.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.9.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.9.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.9.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.9.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.10.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.10.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.10.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.10.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.11.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.11.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.11.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.11.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.12.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.12.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.12.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.12.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.13.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.13.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.13.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.13.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.14.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.14.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.14.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.14.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.15.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.15.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.15.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.15.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleDict'>\n",
      "no weight decay applied to transformer.h.0.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.0.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.0.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.0.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.1.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.1.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.1.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.1.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.2.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.2.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.2.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.2.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.3.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.3.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.3.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.3.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.4.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.4.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.4.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.4.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.5.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.5.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.5.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.5.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.6.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.6.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.6.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.6.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.7.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.7.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.7.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.7.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.8.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.8.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.8.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.8.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.9.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.9.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.9.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.9.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.10.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.10.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.10.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.10.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.11.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.11.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.11.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.11.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.12.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.12.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.12.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.12.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.13.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.13.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.13.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.13.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.14.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.14.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.14.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.14.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.15.attn.c_attn.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.15.attn.c_attn.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.15.attn.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.15.attn.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_fc.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_fc.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_proj.lora.A, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_proj.lora.B, it's a <class 'torch.nn.modules.container.ModuleList'>\n",
      "no weight decay applied to transformer.h.0.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.0.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.0.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.0.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.0.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.0.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.0.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.0.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.0.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.0.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.0.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.0.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.0.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.0.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.0.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.0.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.0.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.1.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.1.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.1.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.1.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.1.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.1.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.1.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.1.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.1.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.1.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.1.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.1.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.1.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.1.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.1.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.1.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.1.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.2.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.2.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.2.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.2.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.2.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.2.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.2.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.2.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.2.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.2.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.2.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.2.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.2.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.2.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.2.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.2.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.2.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.3.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.3.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.3.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.3.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.3.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.3.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.3.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.3.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.3.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.3.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.3.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.3.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.3.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.3.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.3.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.3.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.3.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.4.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.4.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.4.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.4.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.4.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.4.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.4.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.4.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.4.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.4.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.4.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.4.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.4.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.4.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.4.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.4.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.4.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.5.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.5.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.5.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.5.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.5.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.5.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.5.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.5.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.5.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.5.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.5.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.5.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.5.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.5.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.5.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.5.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.5.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.6.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.6.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.6.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.6.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.6.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.6.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.6.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.6.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.6.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.6.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.6.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.6.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.6.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.6.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.6.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.6.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.6.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.7.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.7.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.7.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.7.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.7.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.7.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.7.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.7.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.7.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.7.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.7.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.7.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.7.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.7.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.7.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.7.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.7.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.8.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.8.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.8.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.8.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.8.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.8.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.8.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.8.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.8.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.8.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.8.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.8.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.8.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.8.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.8.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.8.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.8.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.9.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.9.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.9.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.9.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.9.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.9.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.9.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.9.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.9.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.9.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.9.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.9.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.9.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.9.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.9.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.9.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.9.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.10.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.10.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.10.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.10.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.10.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.10.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.10.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.10.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.10.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.10.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.10.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.10.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.10.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.10.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.10.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.10.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.10.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.11.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.11.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.11.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.11.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.11.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.11.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.11.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.11.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.11.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.11.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.11.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.11.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.11.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.11.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.11.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.11.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.11.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.12.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.12.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.12.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.12.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.12.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.12.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.12.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.12.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.12.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.12.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.12.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.12.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.12.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.12.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.12.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.12.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.12.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.13.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.13.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.13.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.13.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.13.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.13.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.13.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.13.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.13.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.13.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.13.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.13.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.13.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.13.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.13.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.13.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.13.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.14.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.14.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.14.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.14.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.14.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.14.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.14.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.14.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.14.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.14.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.14.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.14.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.14.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.14.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.14.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.14.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.14.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.15.attn.c_attn.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.15.attn.c_attn.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.15.attn.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.15.attn.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_fc.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_fc.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_proj.lora.A, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_proj.lora.B, it's a <class 'crystallm._model.Block'>\n",
      "no weight decay applied to transformer.h.15.attn.c_attn.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.15.attn.c_attn.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.15.attn.c_proj.lora.A, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.15.attn.c_proj.lora.B, it's a <class 'crystallm._model.CausalSelfAttention'>\n",
      "no weight decay applied to transformer.h.15.attn.c_attn.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.15.attn.c_attn.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.15.attn.c_attn.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.15.attn.c_attn.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.15.attn.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.15.attn.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.15.attn.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.15.attn.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_fc.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_fc.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_proj.lora.A, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_proj.lora.B, it's a <class 'crystallm._model.MLP'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_fc.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_fc.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_proj.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to transformer.h.15.mlp.c_proj.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to lm_head.lora.A, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to lm_head.lora.B, it's a <class 'crystallm._model.LinearWithLoRA'>\n",
      "no weight decay applied to lm_head.lora.A, it's a <class 'crystallm._model.LoRALayer'>\n",
      "no weight decay applied to lm_head.lora.B, it's a <class 'crystallm._model.LoRALayer'>\n",
      "Initializing optimizer from scratch...\n",
      "Compiling the model (takes a ~minute)...\n",
      "Setting requires_grad for LoRA parameters\n",
      "Total parameters: 204892624\n",
      "Trainable parameters: 1054160 (0.51%)\n",
      "===== Sanity Check: Trainable Parameters =====\n",
      "Frozen: _orig_mod.transformer.wte.weight\n",
      "Frozen: _orig_mod.transformer.wpe.weight\n",
      "Frozen: _orig_mod.transformer.h.0.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.0.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.0.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.0.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.0.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.0.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.0.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.0.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.0.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.0.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.0.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.0.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.0.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.0.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.1.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.1.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.1.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.1.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.1.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.1.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.1.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.1.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.1.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.1.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.1.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.1.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.1.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.1.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.2.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.2.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.2.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.2.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.2.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.2.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.2.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.2.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.2.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.2.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.2.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.2.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.2.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.2.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.3.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.3.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.3.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.3.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.3.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.3.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.3.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.3.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.3.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.3.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.3.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.3.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.3.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.3.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.4.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.4.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.4.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.4.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.4.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.4.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.4.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.4.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.4.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.4.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.4.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.4.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.4.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.4.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.5.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.5.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.5.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.5.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.5.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.5.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.5.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.5.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.5.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.5.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.5.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.5.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.5.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.5.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.6.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.6.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.6.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.6.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.6.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.6.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.6.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.6.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.6.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.6.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.6.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.6.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.6.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.6.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.7.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.7.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.7.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.7.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.7.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.7.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.7.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.7.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.7.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.7.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.7.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.7.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.7.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.7.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.8.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.8.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.8.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.8.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.8.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.8.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.8.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.8.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.8.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.8.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.8.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.8.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.8.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.8.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.9.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.9.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.9.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.9.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.9.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.9.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.9.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.9.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.9.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.9.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.9.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.9.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.9.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.9.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.10.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.10.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.10.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.10.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.10.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.10.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.10.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.10.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.10.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.10.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.10.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.10.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.10.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.10.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.11.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.11.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.11.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.11.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.11.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.11.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.11.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.11.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.11.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.11.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.11.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.11.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.11.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.11.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.12.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.12.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.12.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.12.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.12.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.12.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.12.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.12.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.12.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.12.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.12.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.12.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.12.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.12.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.13.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.13.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.13.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.13.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.13.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.13.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.13.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.13.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.13.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.13.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.13.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.13.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.13.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.13.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.14.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.14.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.14.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.14.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.14.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.14.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.14.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.14.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.14.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.14.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.14.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.14.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.14.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.14.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.15.ln_1.weight\n",
      "Frozen: _orig_mod.transformer.h.15.attn.c_attn.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.15.attn.c_attn.lora.A\n",
      "Trainable: _orig_mod.transformer.h.15.attn.c_attn.lora.B\n",
      "Frozen: _orig_mod.transformer.h.15.attn.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.15.attn.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.15.attn.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.h.15.ln_2.weight\n",
      "Frozen: _orig_mod.transformer.h.15.mlp.c_fc.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.15.mlp.c_fc.lora.A\n",
      "Trainable: _orig_mod.transformer.h.15.mlp.c_fc.lora.B\n",
      "Frozen: _orig_mod.transformer.h.15.mlp.c_proj.linear.weight\n",
      "Trainable: _orig_mod.transformer.h.15.mlp.c_proj.lora.A\n",
      "Trainable: _orig_mod.transformer.h.15.mlp.c_proj.lora.B\n",
      "Frozen: _orig_mod.transformer.ln_f.weight\n",
      "Trainable: _orig_mod.lm_head.lora.A\n",
      "Trainable: _orig_mod.lm_head.lora.B\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muccacbo\u001b[0m (\u001b[33muccacbo-university-college-london-ucl-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/uccacbo/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/uccacbo/CrystaLLM/wandb/run-20241030_153306-hsaphu27\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mBG_large_LoRA1730302386.2568555\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/uccacbo-university-college-london-ucl-/crystallm_CIF_BG_tests\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/uccacbo-university-college-london-ucl-/crystallm_CIF_BG_tests/runs/hsaphu27\u001b[0m\n",
      "Warning: torch._inductor.utils warnings are suppressed\n",
      "best val loss found in ckpt_out_dir: 5.420010566711426\n",
      "step 0: train loss 19.4333, val loss 19.4273\n",
      "===== Sanity Check: Parameter Changes After Update =====\n",
      "Parameter: _orig_mod.transformer.h.0.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.0.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.0.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.0.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.0.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.0.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.0.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.0.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.1.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.1.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.1.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.1.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.1.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.1.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.1.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.1.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.2.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.2.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.2.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.2.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.2.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.2.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.2.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.2.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.3.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.3.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.3.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.3.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.3.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.3.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.3.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.3.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.4.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.4.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.4.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.4.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.4.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.4.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.4.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.4.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.5.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.5.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.5.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.5.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.5.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.5.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.5.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.5.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.6.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.6.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.6.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.6.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.6.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.6.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.6.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.6.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.7.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.7.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.7.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.7.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.7.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.7.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.7.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.7.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.8.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.8.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.8.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.8.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.8.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.8.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.8.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.8.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.9.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.9.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.9.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.9.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.9.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.9.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.9.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.9.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.10.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.10.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.10.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.10.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.10.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.10.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.10.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.10.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.11.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.11.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.11.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.11.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.11.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.11.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.11.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.11.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.12.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.12.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.12.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.12.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.12.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.12.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.12.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.12.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.13.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.13.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.13.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.13.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.13.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.13.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.13.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.13.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.14.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.14.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.14.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.14.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.14.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.14.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.14.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.14.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.15.attn.c_attn.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.15.attn.c_attn.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.15.attn.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.15.attn.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.15.mlp.c_fc.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.15.mlp.c_fc.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.15.mlp.c_proj.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.transformer.h.15.mlp.c_proj.lora.B, Update Norm: 0.0\n",
      "Parameter: _orig_mod.lm_head.lora.A, Update Norm: 0.0\n",
      "Parameter: _orig_mod.lm_head.lora.B, Update Norm: 0.0\n",
      "=========================================================\n",
      "iter 0: loss 18.2231, time 150515.61ms, mfu -100.00%\n",
      "step 2: train loss 18.9245, val loss 18.8952\n",
      "iter 2: loss 18.5238, time 117475.83ms, mfu -100.00%\n",
      "step 4: train loss 18.4071, val loss 18.6746\n",
      "iter 4: loss 17.6261, time 124045.81ms, mfu -100.00%\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python bin/train.py --config=config/finetune_LoRA_BG.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing prompt to prompts/na2cl2.txt ...\n"
     ]
    }
   ],
   "source": [
    "!python bin/make_prompt_file.py Na2Cl2 prompts/na2cl2.txt --spacegroup P4/nmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration:\n",
      "out_dir: finetuned_models/BG_LoRA\n",
      "start: FILE:sampling/prompts/Ti4Ga2Cu2.txt\n",
      "num_samples: 1\n",
      "max_new_tokens: 100\n",
      "temperature: 0.8\n",
      "top_k: 10\n",
      "seed: 1337\n",
      "device: cuda\n",
      "dtype: bfloat16\n",
      "compile: false\n",
      "target: console\n",
      "generated_dir: sampling/test_generated_cifs_BG\n",
      "token_resize: true\n",
      "dataset: CIF_BG_proj/BG_large_tokens\n",
      "\n",
      "Found dataset vocab_size = 372 (inside CIF_BG_proj/BG_large_tokens/meta.pkl)\n",
      "Model configuration:\n",
      "n_layer: 16\n",
      "n_head: 16\n",
      "n_embd: 1024\n",
      "block_size: 2048\n",
      "bias: false\n",
      "vocab_size: 372\n",
      "dropout: 0.1\n",
      "finetune_method: LoRA\n",
      "\n",
      "number of parameters: 201.74M\n",
      "Loaded state_dict with LoRA compatibility.\n",
      "Model loaded successfully.\n",
      "Best validation loss (from checkpoint): 0.2797\n",
      "Training loss not found in checkpoint.\n",
      "Generating samples...\n",
      "data_Ti4Ga2Cu2\n",
      "loop_\n",
      "_atom_type_symbol\n",
      "_atom_type_electronegativity\n",
      "_atom_type_radius\n",
      "_atom_type_ionic_radius\n",
      "Ti 1.5400 1.4000 0.8517\n",
      "Ga 1.8100 1.3000 0.7600\n",
      "Cu 1.9000 1.3500 0.8200\n",
      "_symmetry_space_group_name_H-M Immm\n",
      "_cell_length_a 10.3022\n",
      "_cell_length_b 10.6083\n",
      "_cell_length_c 14.8295\n",
      "_cell_angle_alpha 90.0000\n",
      "_cell_angle_beta 90.0000\n",
      "_cell_angle_gamma 90.0000\n",
      "_symmetry_Int_Tables_number 71\n",
      "_chemical_formula_structural Ti2GaCu\n",
      "_chemical_formula_sum 'Ti4 Ga2 Cu2'\n",
      "_cell_volume 1620.6897\n",
      "_cell_formula_units_Z 2\n",
      "loop_\n",
      "_symmetry_equiv_pos_site_id\n",
      "_symmetry_equiv_pos_as_xyz\n",
      "1 'x, y, z'\n",
      "loop_\n",
      "_atom_site_type_symbol\n",
      "_atom_site_label\n",
      "_atom_site_symmetry_multiplicity\n",
      "_atom_site_fract_x\n",
      "_atom_site_fract_y\n",
      "_atom_site_fract_z\n",
      "_atom_site_occupancy\n",
      "Ti Ti0 4 0.2428 0.0000 0.0000 1.0\n",
      "Ga Ga1 2 0.0000 0.5000 0.5000 1.0\n",
      "Cu Cu2 2 0.0000 0.0000 0.0000 1.0\n",
      "Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:Bandgap_eV:\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python bin/sample.py --config sampling/prompt_config/BG_gen_ft_LoRA.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error post-processing CIF file 'sample_1.cif': could not extract space group from:\n",
      "data_Co4B2Os2\n",
      " loop_ Bandgap_eV: _atom_type_symbol _atom_type_electronegativity _atom_type_radius Co,1z8800,1z3500,0z7683 B,2z0400,0z4500,0z4100 Os,2z2000,1z3000,0z6730 _symmetry_space_group_name_H-M,I4/mcm _cell_length_a,3z6036 _cell_length_b,3z6036 _cell_length_c,6z4358 _cell_angle_alpha,90z0000 _cell_angle_beta,90z0000 _cell_angle_gamma,90z0000 _symmetry_Int_Tables_number,119 _chemical_formula_structural,Co2BOs _chemical_formula_sum,/Co4,B2,Os2/ _cell_volume,83z4773 _cell_formula_units_Z,2 loop_ _symmetry_equiv_pos_site_id _symmetry_equiv_pos_as_xyz 1,/_atom_type_oxidation_number',x',y/ loop_ _atom_site_type_symbol _atom_site_label _atom_site_symmetry_multiplicity _atom_site_fract_x _atom_site_fract_y _atom_site_fract_z _atom_site_occupancy Co,Co0,2,0z0000,0z0000,0z0000,1 Co,Co1,2,0z0000,0z5000,0z7500,1 B,B2,2,0z0000,0z5000,0z2500,1 Os,Os3,2,0z0000,0z0000,0z5000,1  data_Ca4Y4Mg4Al4 loop_ Bandgap_eV: _atom_type_symbol _atom_type_electronegativity _atom_type_radius Ca,1z0000,1z8000,1z1400 Y,1z2200,1z8000,1z0400 Mg,1z3100,1z5000,0z8600 Al,1z6100,1z2500,0z6750 _symmetry_space_group_name_H-M,I4mm _cell_length_a,7z4630 _cell_length_b,7z4630 _cell_length_c,7z4630 _cell_angle_alpha,90z0000 _cell_angle_beta,90z0000 _cell_angle_gamma,90z0000 _symmetry_Int_Tables_number,216 _chemical_formula_structural,CaYMgAl _chemical_formula_sum,/Ca4,Y4,Mg4,Al4/ _cell_volume,415z6619 _cell_formula_units_Z,4 loop_ _symmetry_equiv_pos_site_id _symmetry_equiv_pos_as_xyz 1,/_atom_type_oxidation_number',x',y/ loop_ _atom_site_type_symbol _atom_site_label _atom_site_symmetry_multiplicity _atom_site_fract_x _atom_site_fract_y _atom_site_fract_z _atom_site_occupancy Ca,Ca0,4,0z2500,0z2500,0z7500,1 Y,Y1,4,0z0000,0z0000,0z0000,1 Mg,Mg2,4,0z0000,0z0000,0z5000,1 Al,Al3,4,0z2500,0z2500,0z2500,1  data_Na4Sr2Ta2 loop_ Bandgap_eV: _atom_type_symbol _atom_type_electronegativity _atom_type_radius Na,0z9300,1z8000,1z1600 Sr,0z9500,2z0000,1z3200 Ta,1z5000,1z4500,0z8200 _symmetry_space_group_name_H-M,P6_3/mmc _cell_length_a,4z8011 _cell_length_b,4z8011 _cell_length_c,11z0781 _cell_angle_alpha,90z0000 _cell_angle_beta,90z0000 _cell_angle_gamma,90z0000 _symmetry_Int_Tables_number,139 _chemical_formula_structural,Na2SrTa _chemical_formula_sum,/Na4,Sr2,Ta2/ _cell_volume,255z1175 _cell_formula_units_Z,2 loop_ _symmetry_equiv_pos_site_id _symmetry_equiv_pos_as_xyz 1,/_atom_type_oxidation_number',x',y/ loop_ _atom_site_type_symbol _atom_site_label _atom_site_symmetry_multiplicity _atom_site_fract_x _atom_site_fract_y _atom_site_fract_z _atom_site_occupancy Na,Na0,4,0z0000,0z5000,0z2500,1 Sr,Sr1,2,0z0000,0z0000,0z5000,1 Ta,Ta2,2,0z0000,0z0000,0z0000,1  data_Ba1Sr1Y2 loop_ Bandgap_eV: _atom_type_symbol _atom_type_electronegativity _atom_type_radius Ba,0z8900,2z1500,1z4900 Sr,0z9500,2z0000,1z3200 Y,1z2200,1z8000,1z0400 _symmetry_space_group_name_H-M,P-3 _cell_length_a,3z9668 _cell_length_b,3z9668 _cell_length_c,11z2486 _cell_angle_alpha,90z0000 _cell_angle_beta,90z0000 _cell_angle_gamma,90z0000 _symmetry_Int_Tables_number,99 _chemical_formula_structural,BaSrY2 _chemical_formula_sum,/Ba1,Sr1,Y2/ _cell_volume,176z6431 _cell_formula_units_Z,1 loop_ _symmetry_equiv_pos_site_id _symmetry_equiv_pos_as_xyz 1,/_atom_type_oxidation_number',x',y/ loop_ _atom_site_type_symbol _atom_site_label _atom_site_symmetry_multiplicity _atom_site_fract_x _atom_site_fract_y _atom_site_fract_z _atom_site_occupancy Ba,Ba0,1,0z0000,0z0000,0z4713,1 Sr,Sr1,1,0z5000,0z5000,0z7803,1 Y,Y2,1,0z0000,0z0000,0z0276,1 Y,Y3,1,0z5000,0z5000,0z2207,1  data_Mg4V2Tc2 loop_ Bandgap_eV: _atom_type_symbol _atom_type_electronegativity _atom_type_radius Mg,1z3100,1z5000,0z8600 V,1z6300,1z3500,0z7775 Tc,1z9000,1z3500,0z7417 _symmetry_space_group_name_H-M,P4_2/n _cell_length_a,9z0817 _cell_length_b,2z6948 _cell_length_c,5z3893 _cell_angle_alpha,90z0000 _cell_angle_beta,92z4615 _cell_angle_gamma,90z0000 _symmetry_Int_Tables_number,8 _chemical_formula_structural,Mg2VTc _chemical_formula_sum,/Mg4,V2,Tc2/ _cell_volume,131z7700 _cell_formula_units_Z,2 loop_ _symmetry_equiv_pos_site_id _symmetry_equiv_pos_as_xyz 1,/_atom_type_oxidation_number',x',y/ loop_ _atom_site_type_symbol _atom_site_label _atom_site_symmetry_multiplicity _atom_site_fract_x _atom_site_fract_y _atom_site_fract_z _atom_site_occupancy Mg,Mg0,2,0z0005,0z0000,0z9990,1 Mg,Mg1,2,0z2481,0z0000,0z7430,1 V,V2,2,0z2465,0z5000,0z2776,1 Tc,Tc3,2,0z0049,0z5000,0z4804,1  data_La2Mn4Cd2 loop_ Bandgap_eV: _atom_type_symbol _atom_type_electronegativity _atom_type_radius La,1z1000,1z9500,1z1720 Mn,1z5500,1z4000,0z6483 Cd,1z6900,1z5500,1z0900 _symmetry_space_group_name_H-M,P4/mmm _cell_length_a,3z3350 _cell_length_b,4z5894 _cell_length_c,10z5600 _cell_angle_alpha,90z0000 _cell_angle_beta,90z0000 _cell_angle_gamma,90z0000 _symmetry_Int_Tables_number,44 _chemical_formula_structural,LaMn2Cd _chemical_formula_sum,/La2,Mn4,Cd2/ _cell_volume,161z4126 _cell_formula_units_Z,2 loop_ _symmetry_equiv_pos_site_id _symmetry_equiv_pos_as_xyz 1,/_atom_type_oxidation_number',x',y/ loop_ _atom_site_type_symbol _atom_site_label _atom_site_symmetry_multiplicity _atom_site_fract_x _atom_site_fract_y _atom_site_fract_z _atom_site_occupancy La,La0,2,0z0000,0z0000,0z7325,1 Mn,Mn1,2,0z0000,0z0000,0z3191,1 Mn,Mn2,2,0z0000,0z5000,0z9336,1 Cd,Cd3,2,0z0000,0z5000,0z5148,1  data_Ca2V1Ru1 loop_ Bandgap_eV: _atom_type_symbol _atom_type_electronegativity _atom_type_radius Ca,1z0000,1z8000,1z1400 V,1z6300,1z3500,0z7775 Ru,2z2000,1z3000,0z6610 _symmetry_space_group_name_H-M,Fmmm _cell_length_a,3z0762 _cell_length_b,4z8127 _cell_length_c,5z6015 _cell_angle_alpha,90z0000 _cell_angle_beta,97z9498 _cell_angle_gamma,90z0000 _symmetry_Int_Tables_number,10 _chemical_formula_structural,Ca2VRu _chemical_formula_sum,/Ca2,V1,Ru1/ _cell_volume,82z1139 _cell_formula_units_Z,1 loop_ _symmetry_equiv_pos_site_id _symmetry_equiv_pos_as_xyz 1,/_atom_type_oxidation_number',x',y/ loop_ _atom_site_type_symbol _atom_site_label _atom_site_symmetry_multiplicity _atom_site_fract_x _atom_site_fract_y _atom_site_fract_z _atom_site_occupancy Ca,Ca0,2,0z0512,0z0000,0z7584,1 V,V1,1,0z5000,0z5000,0z5000,1 Ru,Ru2,1,0z5000,0z5000,0z0000,1  data_Hf4Ta8As4 loop_ Bandgap_eV: _atom_type_symbol _atom_type_electronegativity _atom_type_radius Hf,1z3000,1z5500,0z8500 Ta,1z5000,1z4500,0z8200 As,2z1800,1z1500,0z6600 _symmetry_space_group_name_H-M,I4mm _cell_length_a,6z6243 _cell_length_b,6z6243 _cell_length_c,6z6243 _cell_angle_alpha,90z0000 _cell_angle_beta,90z0000 _cell_angle_gamma,90z0000 _symmetry_Int_Tables_number,216 _chemical_formula_structural,HfTa2As _chemical_formula_sum,/Hf4,Ta8,As4/ _cell_volume,290z6766 _cell_formula_units_Z,4 loop_ _symmetry_equiv_pos_site_id _symmetry_equiv_pos_as_xyz 1,/_atom_type_oxidation_number',x',y/ loop_ _atom_site_type_symbol _atom_site_label _atom_site_symmetry_multiplicity _atom_site_fract_x _atom_site_fract_y _atom_site_fract_z _atom_site_occupancy Hf,Hf0,4,0z2500,0z2500,0z7500,1 Ta,Ta1,4,0z0000,0z0000,0z0000,1 Ta,Ta2,4,0z2500,0z2500,0z2500,1 As,As3,4,0z0000,0z0000,0z5000,1  data_Li8Mn2Ir2O12 loop_ Bandgap_eV: _atom_type_symbol _atom_type_electronegativity _atom_type_radius Li,0z9800,1z4500,0z9000 Mn,1z5500,1z4000,0z6483 Ir,2z2000,1z3500,0z7650 O,3z4400,0z6000,1z2600 _symmetry_space_group_name_H-M,P6_3/mcm _cell_length_a,5z0354 _cell_length_b,8z7606 _cell_length_c,5z0404 _cell_angle_alpha,90z0000 _cell_angle_beta,109z7540 _cell_angle_gamma,90z0000 _symmetry_Int_Tables_number,5 _chemical_formula_structural,Li4MnIrO6 _chemical_formula_sum,/Li8,Mn2,Ir2,O12/ _cell_volume,208z4915 _cell_formula_units_Z,2 loop_ _symmetry_equiv_pos_site_id _symmetry_equiv_pos_as_xyz 1,/_atom_type_oxidation_number',x',y/ loop_ _atom_site_type_symbol _atom_site_label _atom_site_symmetry_multiplicity _atom_site_fract_x _atom_site_fract_y _atom_site_fract_z _atom_site_occupancy Li,Li0,2,0z0000,0z0002,0z5000,1 Li,Li1,2,\n",
      "processed: sample_1.cif\n",
      "processed: sample_5.cif\n"
     ]
    }
   ],
   "source": [
    "!python bin/postprocess.py test_generated_cifs_BG my_processed_cifs_BG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change default directory of jupyter notebook so every time I start it it's in the right place\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "1. could be that there is no end to the genertation because somehow in the sample.py file the condition to terminate is not found\n",
    "    \n",
    "    a. even the max tokens isnt used\n",
    "\n",
    "2. Maybe when adding the dimnension I didnt add it correctly so all the dimensions got mumble jumbled so the weights and parameter weights arent assigned to the right thing yet (LoRA could fix this? - but for now could try the token resize thing)\n",
    "\n",
    "3. Maybe need to retrtain all params from scratch\n",
    "\n",
    "4. Try to assign the start indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lora Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loralib\n",
      "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: loralib\n",
      "Successfully installed loralib-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install loralib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 201.74M\n"
     ]
    }
   ],
   "source": [
    "import loralib\n",
    "import os\n",
    "import torch\n",
    "from crystallm import GPT, GPTConfig\n",
    "\n",
    "out_dir = 'pretrained_models/large_model_untouched'\n",
    "model_args = dict(n_layer=12, n_head=12, n_embd=768, block_size=1024,\n",
    "                      bias=False, vocab_size=None, dropout=0.1)\n",
    "\n",
    "ckpt_path = os.path.join(out_dir, \"ckpt.pt\")\n",
    "checkpoint = torch.load(ckpt_path, map_location='cuda')\n",
    "checkpoint_model_args = checkpoint[\"model_args\"]\n",
    "\n",
    "# Update model arguments based on checkpoint\n",
    "for k in [\"n_layer\", \"n_head\", \"n_embd\", \"block_size\", \"bias\", \"vocab_size\"]:\n",
    "    model_args[k] = checkpoint_model_args[k]\n",
    "\n",
    "gptconf = GPTConfig(**model_args)\n",
    "model = GPT(gptconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(371, 1024)\n",
      "    (wpe): Embedding(2048, 1024)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-15): 16 x Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (c_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=371, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearWithLoRA' object has no attribute 'in_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m assign_lora \u001b[38;5;241m=\u001b[39m partial(LinearWithLoRA, rank\u001b[38;5;241m=\u001b[39mlora_r, alpha\u001b[38;5;241m=\u001b[39mlora_alpha)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m---> 17\u001b[0m     h\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mc_attn \u001b[38;5;241m=\u001b[39m \u001b[43massign_lora\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_attn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     h\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mc_proj \u001b[38;5;241m=\u001b[39m assign_lora(h\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mc_proj)\n\u001b[1;32m     19\u001b[0m     h\u001b[38;5;241m.\u001b[39mmlp\u001b[38;5;241m.\u001b[39mc_fc \u001b[38;5;241m=\u001b[39m assign_lora(h\u001b[38;5;241m.\u001b[39mmlp\u001b[38;5;241m.\u001b[39mc_fc)\n",
      "Cell \u001b[0;32mIn[42], line 6\u001b[0m, in \u001b[0;36mLinearWithLoRA.__init__\u001b[0;34m(self, linear, rank, alpha)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear \u001b[38;5;241m=\u001b[39m linear\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora \u001b[38;5;241m=\u001b[39m LoRALayer(\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mlinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_features\u001b[49m, linear\u001b[38;5;241m.\u001b[39mout_features, rank, alpha\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/crystallm_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LinearWithLoRA' object has no attribute 'in_features'"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "# default hyperparameter choices\n",
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "lora_attn = True\n",
    "lora_proj = True\n",
    "lora_head = True\n",
    "\n",
    "\n",
    "layers = []\n",
    "\n",
    "assign_lora = partial(LinearWithLoRA, rank=lora_r, alpha=lora_alpha)\n",
    "\n",
    "for h in model.transformer.h:\n",
    "    h.attn.c_attn = assign_lora(h.attn.c_attn)\n",
    "    h.attn.c_proj = assign_lora(h.attn.c_proj)\n",
    "    h.mlp.c_fc = assign_lora(h.mlp.c_fc)\n",
    "    h.mlp.c_proj = assign_lora(h.mlp.c_proj)\n",
    "if lora_head:\n",
    "    model.transformer.lm_head = assign_lora(model.GPT.lm_head)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSync files to here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authenticity of host 'myriad.rc.ucl.ac.uk (193.60.252.107)' can't be established.\n",
      "ED25519 key fingerprint is SHA256:waqBYWAb+g1lwUGWz8ku4M48McIBWGCdpMoU8l8j6tU.\n",
      "This key is not known by any other names.\n",
      "Are you sure you want to continue connecting (yes/no/[fingerprint])? ^C\n"
     ]
    }
   ],
   "source": [
    "!rsync -avz --progress \\\n",
    "uccacbo@myriad.rc.ucl.ac.uk:/home/uccacbo/CrystaLLM/BG_CIF_dataset.tar.gz /home/uccacbo/CrystaLLM/BG_datasets/BG_CIF_dataset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /home/uccacbo/miniconda3/envs/crystallm_venv/lib/python3.10/site-packages (6.0.0)\n",
      "Collecting pynvml\n",
      "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
      "Downloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
      "Installing collected packages: pynvml\n",
      "Successfully installed pynvml-11.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil pynvml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystallm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
