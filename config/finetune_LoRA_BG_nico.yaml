# directories and checkpoints
out_dir: "pretrained_models/large_model_untouched" # path to model ckpt to train on
dataset: "CIF_BG_proj/BG_large_tokens"
init_from: "resume"
finetune_method : "LoRA"
ckpt_out_dir: "finetuned_models/BG_LoRA"  # the path to the directory to save the checkpoints

# evaluation frequency
always_save_checkpoint: False
validate: True
eval_interval: 50
eval_iters_train: 50
eval_iters_val: 50
log_interval: 10  # how often to print to the console (1 = every iteration)
sanity_check: False  # if True, prints details of parameter freezing and tensor size changes

# batch and block sizes
gradient_accumulation_steps: 8
batch_size: 8
block_size: 2048  # context of up to `block_size` previous characters

# architecture
n_layer: 14
n_head: 14
n_embd: 1024
dropout: 0.1

# trackers
codecarbon: True  # if True, log emissions to CodeCarbon
tracker_project: "crystallm"  # the name of the project in the CodeCarbon dashboard
metrics_dir: "comp_metrics"  # the path to the folder where the metrics will be stored
CarbonTracker: False  # if True, log emissions to CarbonTracker

#wandb
wandb_log: True # disabled by default
wandb_project: 'crystallm_CIF_BG'
wandb_run_name: 'BG_large_LoRA'

# learning rate and optimizer
learning_rate: 1e-3
decay_lr: True
min_lr: 1e-4  # learning_rate / 10 usually
beta2: 0.99  # make a bit bigger because number of tokens per iter is small

# training iterations
max_iters: 2500
lr_decay_iters: 2500  # make equal to max_iters usually
warmup_iters: 100  # not super necessary potentially

# on macbook also add
# device: 'cpu'  # run on cpu only
# compile: False # do not torch compile the model
