# directories and checkpoints
out_dir: "model_ckpts/pretrained_models/small_model_untouched" # path to model ckpt to train on
dataset: "CIF_BG_proj/BG_large_tokens"
init_from: "resume"
finetune_method : "finetune_head"
ckpt_out_dir: "model_ckpts/finetuned_models/BG_head_debug"  # the path to the directory to save the checkpoints
adaptation: "cifextd"

# evaluation frequency
always_save_checkpoint: true
validate: True
eval_interval: 1
eval_iters_train: 1
eval_iters_val: 1
log_interval: 1  # how often to print to the console (1 = every iteration)
sanity_check: True  # if True, prints resizing of tensor details

# batch and block sizes
gradient_accumulation_steps: 4
batch_size: 4
block_size: 1024  # context of up to `block_size` previous characters

# architecture
n_layer: 16
n_head: 16
n_embd: 1024
dropout: 0.1

# trackers
codecarbon: False  # if True, log emissions to CodeCarbon
tracker_project: "crystallm"  # the name of the project in the CodeCarbon dashboard
metrics_dir: "comp_metrics"  # the path to the folder where the metrics will be stored

#wandb
wandb_log: False # disabled by default
wandb_project: 'crystallm_CIF_BG_tests'
wandb_run_name: 'BG_large_head'

# learning rate and optimizer
learning_rate: 1e-4
decay_lr: True
lr_decay_iters: 2  # make equal to max_iters usually
min_lr: 1e-5  # learning_rate / 10 usually
beta2: 0.99  # make a bit bigger because number of tokens per iter is small

# training iterations
max_iters: 1
warmup_iters: 100  # not super necessary potentially

# on macbook also add
# device: 'cpu'  # run on cpu only
# compile: False # do not torch compile the model
