# directories and checkpoints
out_dir: "model_ckpts/pretrained_models/large_model_untouched" # path to model ckpt to train on
dataset: "CIF_BG_proj/BG_large_tokens_excl_0BG"
init_from: "resume"
finetune_method: "finetune_head"
ckpt_out_dir: "model_ckpts/finetuned_models/BG_head"  # the path to the directory to save the checkpoints
adaptation: "cifextd"  # the type of adaptation to perform

# evaluation frequency
always_save_checkpoint: false
validate: true
eval_interval: 100
eval_iters_train: 100
eval_iters_val: 100
log_interval: 50  # how often to print to the console (1 = every iteration)
sanity_check: false  # if True, prints resizing of tensor details

# batch and block sizes
gradient_accumulation_steps: 8
batch_size: 8
block_size: 2048  # context of up to `block_size` previous characters

# architecture
n_layer: 16
n_head: 16
n_embd: 1024
dropout: 0.1

# trackers
codecarbon: true  # if True, log emissions to CodeCarbon
tracker_project: "crystallm"  # the name of the project in the CodeCarbon dashboard
metrics_dir: "comp_metrics"  # the path to the folder where the metrics will be stored

#wandb
wandb_log: true # disabled by default
wandb_project: 'crystallm_CIF_BG_v4'
wandb_run_name: 'finetune_head_BG_nico'

# learning rate and optimizer
learning_rate: 1e-4
decay_lr: true
lr_decay_iters: 3000  # make equal to max_iters usually
min_lr: 1e-5  # learning_rate / 10 usually
beta2: 0.995 # make a bit bigger because number of tokens per iter is small

# training iterations
max_iters: 3000
warmup_iters: 100  # not super necessary potentially

# on macbook also add
# device: 'cpu'  # run on cpu only
# compile: False # do not torch compile the model
weight_decay: 0.1
LoRA_rank: 2
LoRA_alpha: 4
